# Scraping Package Environment Variables
# Copy relevant variables to your app's .env.local file

# =============================================================================
# AI EXTRACTION SERVICES (Optional)
# =============================================================================

# Anthropic API Key (for AI-powered content extraction)
# Get from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# OpenAI API Key (for AI-powered content extraction)
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here

# =============================================================================
# SCRAPING CONFIGURATION (Optional)
# =============================================================================

# Environment settings (affects browser configuration)
NODE_ENV=development

# Enable debug mode for detailed logging
SCRAPING_DEBUG=true

# Default timeout for scraping operations (milliseconds)
SCRAPING_TIMEOUT=30000

# Maximum retries for failed scraping attempts
SCRAPING_MAX_RETRIES=3

# =============================================================================
# BROWSER CONFIGURATION (Optional)
# =============================================================================

# Browser provider (playwright, puppeteer, hero)
SCRAPING_BROWSER_PROVIDER=playwright

# Headless mode (true for production, false for development)
SCRAPING_HEADLESS=true

# Browser viewport settings
SCRAPING_VIEWPORT_WIDTH=1920
SCRAPING_VIEWPORT_HEIGHT=1080

# =============================================================================
# RATE LIMITING (Optional)
# =============================================================================

# Requests per minute for scraping operations
SCRAPING_RATE_LIMIT=60

# Delay between requests (milliseconds)
SCRAPING_REQUEST_DELAY=1000

# =============================================================================
# PROXY CONFIGURATION (Optional)
# =============================================================================

# HTTP proxy for scraping requests
SCRAPING_PROXY_URL=http://proxy.example.com:8080

# Proxy authentication
SCRAPING_PROXY_USERNAME=your-proxy-username
SCRAPING_PROXY_PASSWORD=your-proxy-password

# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================

# 1. Basic Scraping (No AI):
#    - No environment variables required
#    - Package works with default browser providers
#    - Supports: Playwright, Puppeteer, Cheerio, Fetch

# 2. AI-Enhanced Extraction (Optional):
#    - Add API keys for AI services
#    - Enables structured data extraction
#    - Intelligent content parsing
#    - Schema-based validation

# 3. Browser Providers:
#    Playwright (Recommended):
#    ✅ Best performance and reliability
#    ✅ Multiple browser engines (Chromium, Firefox, Safari)
#    ✅ Advanced automation features
#    
#    Puppeteer:
#    ✅ Popular and well-documented
#    ✅ Chrome/Chromium focus
#    ✅ Good ecosystem support
#    
#    Hero:
#    ✅ Stealth mode capabilities
#    ✅ Advanced anti-detection
#    ✅ Cloud-based option

# 4. Usage Examples:
#    ```typescript
#    import { createScraper } from '@repo/scraping/server';
#    
#    // Basic HTML scraping
#    const scraper = createScraper({
#      provider: 'playwright',
#      headless: true,
#    });
#    
#    const result = await scraper.scrape('https://example.com');
#    
#    // AI-powered extraction (requires API keys)
#    const data = await scraper.extractWithAI(html, 'Extract product information');
#    ```

# 5. Provider Selection Guide:
#    Simple HTML sites: Use 'cheerio' or 'fetch'
#    JavaScript-heavy sites: Use 'playwright' or 'puppeteer'
#    Anti-bot protection: Use 'hero' with stealth options
#    High-volume scraping: Use 'playwright' with proxy rotation

# 6. Performance Optimization:
#    - Use headless mode in production
#    - Implement request rate limiting
#    - Cache frequently accessed content
#    - Use browser pooling for multiple requests
#    - Disable images/CSS for faster loading

# 7. Legal and Ethical Considerations:
#    - Respect robots.txt files
#    - Implement proper rate limiting
#    - Don't overload target servers
#    - Check website terms of service
#    - Consider using official APIs when available

# 8. Common Scraping Patterns:
#    Product data extraction
#    News article aggregation
#    Price monitoring
#    Social media content
#    Real estate listings
#    Job postings

# 9. Error Handling:
#    - Network timeouts
#    - Anti-bot detection
#    - Rate limiting
#    - Content changes
#    - JavaScript rendering issues

# 10. Browser Security:
#     - Run in sandboxed environment
#     - Disable unnecessary features
#     - Use minimal permissions
#     - Regular browser updates
#     - Monitor for malicious content