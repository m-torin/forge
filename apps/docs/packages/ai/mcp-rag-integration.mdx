---
title: MCP-RAG Integration Usage Guide
description:
  Complete guide for combining Model Context Protocol (MCP) tools with
  Retrieval-Augmented Generation (RAG) workflows
icon: "search"
---

# MCP-RAG Integration Usage Guide

This guide demonstrates how to create powerful AI applications by combining:

- **MCP Tools**: Real-time data access via web search, filesystem, databases
- **RAG Systems**: Knowledge base retrieval with vector embeddings
- **AI SDK v5**: Modern streaming and tool calling patterns

## üöÄ Quick Start

### Basic Setup

```typescript
import {
  createMCPToolsFromConfigs,
  createRAGWorkflow,
  createUpstashVectorDB
} from "@repo/ai/server";

// Set up MCP tools
const mcpConfigs = [
  {
    name: "perplexity-search",
    transport: {
      type: "stdio",
      command: "npx",
      args: ["@perplexity/mcp-server"]
    }
  }
];

// Set up RAG system
const vectorDB = createUpstashVectorDB({
  url: process.env.UPSTASH_VECTOR_REST_URL!,
  token: process.env.UPSTASH_VECTOR_REST_TOKEN!,
  namespace: "my-knowledge-base"
});

const ragWorkflow = createRAGWorkflow({
  vectorDB,
  embeddingModel: "text-embedding-3-small",
  chatModel: "gpt-4o",
  topK: 5,
  enableReranking: true
});
```

### Simple Web-Enhanced RAG Query

```typescript
async function enhancedQuery(question: string) {
  // Get MCP tools
  const { tools, closeAllClients } =
    await createMCPToolsFromConfigs(mcpConfigs);

  try {
    // 1. Search knowledge base
    const ragContext = await ragWorkflow.retrieveContext(question);

    // 2. Get live web information
    const webResult = await generateText({
      model: openai("gpt-4o"),
      tools,
      prompt: `Search for latest information: ${question}`,
      maxSteps: 3
    });

    // 3. Combine and answer
    const answer = await generateText({
      model: openai("gpt-4o"),
      prompt: `
        Question: ${question}

        Knowledge Base: ${ragContext.map((c) => c.content).join("

")}

        Latest Information: ${webResult.text}

        Provide a comprehensive answer combining both sources.
      `
    });

    return answer.text;
  } finally {
    await closeAllClients();
  }
}
```

## üìã Usage Patterns

### 1. Web Search Enhanced RAG üîç

**Use Case**: Combine existing knowledge with real-time web information

```typescript
import { webSearchEnhancedRAG } from "@repo/ai/examples/mcp-rag-integration-examples";

const result = await webSearchEnhancedRAG();
console.log(
  `Query processed with ${result.ragContext} KB docs and ${result.webSearchSteps} web searches`
);
```

**Best For**:

- Current events + historical context
- Fact-checking with latest information
- Research with time-sensitive data

**Performance**: ~2-5 seconds per query

### 2. Research Assistant Workflow üî¨

**Use Case**: Multi-step research with document management

```typescript
import { researchAssistantWorkflow } from "@repo/ai/examples/mcp-rag-integration-examples";

const research = await researchAssistantWorkflow("AI Ethics and Safety");
console.log(`Research completed: ${research.documentsAdded} docs added to KB`);
```

**Features**:

- ‚úÖ Automated research planning
- ‚úÖ Multi-source information gathering
- ‚úÖ Knowledge base population
- ‚úÖ Comprehensive analysis and synthesis
- ‚úÖ Performance tracking

### 3. Real-time Knowledge Updates üîÑ

**Use Case**: Keep knowledge base current with latest developments

```typescript
import { realTimeKnowledgeUpdate } from "@repo/ai/examples/mcp-rag-integration-examples";

const update = await realTimeKnowledgeUpdate("Machine Learning");
console.log(
  `Updated KB: ${update.initialVectors} ‚Üí ${update.finalVectors} vectors`
);
```

**Features**:

- ‚úÖ Novelty detection
- ‚úÖ Incremental updates
- ‚úÖ Change tracking
- ‚úÖ Confidence scoring

### 4. Streaming Chat Interface üí¨

**Use Case**: Real-time conversational AI with live data

```typescript
import { streamingMCPRAGChat } from "@repo/ai/examples/mcp-rag-integration-examples";

for await (const chunk of streamingMCPRAGChat("What are current AI trends?", {
  domain: "AI Research",
  namespace: "trends"
})) {
  console.log(`[${chunk.type}]:`, chunk.data);
}
```

**Stream Types**:

- `mcp-tools`: Tool execution status
- `rag-context`: Knowledge base retrieval
- `synthesis`: Answer generation
- `metadata`: Performance metrics

## üõ† Configuration Guide

### MCP Server Setup

#### Perplexity Web Search

```typescript
{
  name: 'perplexity-search',
  transport: {
    type: 'stdio',
    command: 'npx',
    args: ['@perplexity/mcp-server'],
  },
}
```

#### Filesystem Access

```typescript
{
  name: 'filesystem',
  transport: {
    type: 'stdio',
    command: 'npx',
    args: ['@mcp/server-filesystem', '/path/to/documents'],
  },
}
```

#### SQLite Database

```typescript
{
  name: 'sqlite',
  transport: {
    type: 'stdio',
    command: 'npx',
    args: ['@mcp/server-sqlite', '/path/to/database.db'],
  },
}
```

### RAG Configuration Options

```typescript
const ragConfig = {
  // Vector Database
  vectorDB: createUpstashVectorDB({ ... }),

  // Models
  embeddingModel: 'text-embedding-3-small', // or 'text-embedding-3-large'
  chatModel: 'gpt-4o', // or 'claude-3-5-sonnet-20241022'
  provider: 'openai', // or 'anthropic'

  // Retrieval Settings
  topK: 5,                    // Number of documents to retrieve
  similarityThreshold: 0.7,   // Minimum similarity score
  maxContextLength: 4000,     // Maximum context length

  // Advanced Features
  enableReranking: true,      // Improve result ranking
  enableCaching: true,        // Cache frequent queries

  // Namespace for multi-tenant scenarios
  namespace: 'my-knowledge-domain',
};
```

### Performance Tuning

| Setting               | Default | Performance Impact                   | Recommended      |
| --------------------- | ------- | ------------------------------------ | ---------------- |
| `topK`                | 5       | Higher = more context, slower        | 3-10             |
| `similarityThreshold` | 0.7     | Higher = fewer results, faster       | 0.6-0.8          |
| `maxContextLength`    | 4000    | Higher = better context, more tokens | 2000-8000        |
| `enableCaching`       | true    | Much faster for repeated queries     | true             |
| `enableReranking`     | false   | Better relevance, slower             | true for quality |

## üìä Monitoring & Analytics

### Built-in Metrics

```typescript
// RAG workflow statistics
const stats = await ragWorkflow.getStats();
console.log({
  totalVectors: stats.totalVectors,
  namespace: stats.namespace,
  cacheSize: stats.cacheSize,
  dimension: stats.dimension
});

// Query performance
const result = await ragWorkflow.query(question);
console.log({
  confidence: result.confidence,
  sourcesUsed: result.sources.length,
  tokensUsed: result.tokensUsed
});
```

### Custom Telemetry

```typescript
import { generateText } from "ai";

const result = await generateText({
  model: openai("gpt-4o"),
  tools: mcpTools,
  prompt: question,
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      workflowType: "mcp-rag",
      domain: "research",
      userId: "user-123"
    }
  }
});
```

## üîß Advanced Patterns

### Multi-Domain Knowledge Base

```typescript
class MultiDomainRAG {
  private domains = new Map<string, any>();

  async addDomain(name: string, namespace: string) {
    const vectorDB = createUpstashVectorDB({
      url: process.env.UPSTASH_VECTOR_REST_URL!,
      token: process.env.UPSTASH_VECTOR_REST_TOKEN!,
      namespace
    });

    const workflow = createRAGWorkflow({ vectorDB, namespace });
    this.domains.set(name, workflow);
  }

  async queryAcrossDomains(question: string, domains: string[]) {
    const results = await Promise.all(
      domains.map(async (domain) => {
        const workflow = this.domains.get(domain);
        if (!workflow) return null;

        const result = await workflow.query(question);
        return { domain, ...result };
      })
    );

    return results.filter(Boolean);
  }
}
```

### Conversation Memory

```typescript
class ConversationalRAG {
  private conversationHistory: Array<{
    question: string;
    answer: string;
    context: any[];
    timestamp: Date;
  }> = [];

  async query(question: string, mcpTools: any) {
    // Include conversation context
    const contextualQuestion =
      this.conversationHistory.length > 0
        ? `Previous conversation:
         ${this.conversationHistory
           .slice(-3)
           .map((h) => `Q: ${h.question}
A: ${h.answer}`)
           .join("

")}

         Current question: ${question}`
        : question;

    const result = await ragWorkflow.query(contextualQuestion);

    // Store in conversation history
    this.conversationHistory.push({
      question,
      answer: result.answer,
      context: result.context,
      timestamp: new Date()
    });

    return result;
  }

  clearHistory() {
    this.conversationHistory = [];
  }
}
```

## üß™ Testing Your Implementation

### Unit Testing

```typescript
import { AITestFactory } from "@repo/ai/__tests__/ai-test-factory";

describe("MCP-RAG Integration", () => {
  test("should combine web search with RAG context", async () => {
    const env = AITestFactory.createBasicTestEnvironment({
      responses: ["Combined web and RAG response"]
    });

    // Mock RAG context
    const mockContext = [
      { id: "doc1", content: "Historical context", score: 0.8, metadata: {} }
    ];

    // Test your workflow
    const result = await yourMCPRAGFunction("test query");
    expect(result).toContain("Combined");

    env.cleanup();
  });
});
```

## üö® Error Handling & Best Practices

### Graceful Degradation

```typescript
async function robustMCPRAG(question: string) {
  try {
    // Try full MCP-RAG workflow
    return await webSearchEnhancedRAG();
  } catch (mcpError) {
    console.warn("MCP tools unavailable, falling back to RAG-only");

    try {
      // Fallback to RAG-only
      return await ragWorkflow.query(question);
    } catch (ragError) {
      console.error("Both MCP and RAG failed, using basic AI response");

      // Final fallback to basic AI
      return await generateText({
        model: openai("gpt-4o"),
        prompt: `Answer this question to the best of your ability: ${question}`
      });
    }
  }
}
```

### Resource Management

```typescript
class ManagedMCPRAG {
  private clients: any[] = [];
  private ragWorkflow: any;

  async initialize(mcpConfigs: any[], ragConfig: any) {
    const { tools, clients, closeAllClients } =
      await createMCPToolsFromConfigs(mcpConfigs);
    this.clients = clients;
    this.cleanup = closeAllClients;
    this.ragWorkflow = createRAGWorkflow(ragConfig);
  }

  async query(question: string) {
    // Your MCP-RAG logic here
  }

  async cleanup() {
    await this.closeAllClients?.();
    this.ragWorkflow?.clearCache();
  }
}

// Usage with proper cleanup
const manager = new ManagedMCPRAG();
try {
  await manager.initialize(mcpConfigs, ragConfig);
  const result = await manager.query("test question");
} finally {
  await manager.cleanup();
}
```

## üìà Production Deployment

### Environment Configuration

```bash
# Required Environment Variables
UPSTASH_VECTOR_REST_URL=https://your-vector-db.upstash.io
UPSTASH_VECTOR_REST_TOKEN=your-token
PERPLEXITY_API_KEY=your-api-key
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key

# Optional Configuration
MCP_GRACEFUL_DEGRADATION=true
RAG_CACHE_ENABLED=true
RAG_MAX_CONTEXT_LENGTH=4000
```

### Scaling Considerations

| Component            | Scaling Strategy      | Notes                             |
| -------------------- | --------------------- | --------------------------------- |
| MCP Connections      | Connection pooling    | Reuse connections across requests |
| Vector Database      | Sharding by namespace | Separate domains/tenants          |
| Embedding Generation | Batch processing      | Process multiple docs together    |
| Response Caching     | Redis/Memory cache    | Cache frequent queries            |

### Monitoring in Production

```typescript
import { logInfo, logError } from "@repo/observability/server/next";

const productionMCPRAG = async (question: string) => {
  const startTime = Date.now();

  try {
    logInfo("MCP-RAG query started", {
      operation: "mcp_rag_query",
      question: question.substring(0, 100)
    });

    const result = await webSearchEnhancedRAG();

    logInfo("MCP-RAG query completed", {
      operation: "mcp_rag_query",
      duration: Date.now() - startTime,
      tokensUsed: result.tokensUsed,
      confidence: result.confidence
    });

    return result;
  } catch (error) {
    logError("MCP-RAG query failed", {
      operation: "mcp_rag_query",
      question: question.substring(0, 100),
      duration: Date.now() - startTime,
      error: error instanceof Error ? error : new Error(String(error))
    });
    throw error;
  }
};
```

<Note>
  This guide provides comprehensive coverage of MCP-RAG integration patterns.
  Start with the Quick Start section and gradually explore more advanced
  patterns as your use case evolves.
</Note>
