---
title: Advanced Streaming
description:
  Production-ready streaming with rich metadata tracking, interruption control,
  backpressure handling, and flow management for high-performance AI
  applications
icon: activity
---

# Advanced Streaming

Build high-performance AI applications with advanced streaming capabilities
including rich metadata tracking, stream interruption/resumption, intelligent
backpressure handling, and sophisticated flow control mechanisms.

## Overview

Advanced streaming provides enterprise-grade streaming capabilities beyond basic
AI SDK functionality. It includes comprehensive metadata tracking, stream
control mechanisms, backpressure handling, and performance optimization features
for production AI applications.

## Enhanced Stream Data

### Typed Stream Data

Create streaming responses with rich, typed metadata:

```typescript
import {
  EnhancedStreamData,
  createStreamingResponseWithData
} from "@repo/ai/server/streaming";

const streamData = new EnhancedStreamData({
  types: {
    progress: {
      step: "string",
      percentage: "number",
      estimated_time: "number"
    },
    metrics: {
      tokens_used: "number",
      cost: "number",
      model: "string"
    },
    source: {
      id: "string",
      title: "string",
      relevance: "number",
      url: "string?"
    }
  }
});

// Stream with typed data
const response = await createStreamingResponseWithData({
  model: createAnthropicModel("claude-3-5-sonnet-20241022"),
  messages: [{ role: "user", content: "Research renewable energy trends" }],
  streamData,
  onProgress: ({ step, percentage }) => {
    streamData.append({
      type: "progress",
      data: { step, percentage, estimated_time: (100 - percentage) * 2 }
    });
  },
  onSource: (source) => {
    streamData.append({
      type: "source",
      data: source
    });
  },
  onMetrics: (metrics) => {
    streamData.append({
      type: "metrics",
      data: metrics
    });
  }
});
```

### Stream Data Patterns

Pre-built patterns for common streaming scenarios:

```typescript
import { streamDataPatterns } from "@repo/ai/server/streaming";

// Research workflow with sources
const researchStream = streamDataPatterns.createResearchStream({
  includeProgressTracking: true,
  includeCostTracking: true,
  sourceMetadata: ["relevance", "credibility", "date"]
});

// Code generation with compilation
const codeStream = streamDataPatterns.createCodeGenerationStream({
  includeSyntaxValidation: true,
  includeTestResults: true,
  compilerOutput: true
});

// Analysis with confidence scoring
const analysisStream = streamDataPatterns.createAnalysisStream({
  includeConfidenceScores: true,
  includeDataSources: true,
  trackReasoningSteps: true
});
```

### Stream Data Utilities

Helpful utilities for stream processing:

```typescript
import { streamDataUtils } from "@repo/ai/server/streaming";

// Transform stream data
const transformedStream = streamDataUtils.transform(originalStream, {
  filterTypes: ["progress", "metrics"],
  aggregateMetrics: true,
  batchUpdates: { interval: 500, maxBatch: 10 }
});

// Merge multiple streams
const mergedStream = streamDataUtils.merge(
  [researchStream, analysisStream, summaryStream],
  {
    mergeStrategy: "interleave",
    priorityOrder: ["progress", "source", "metrics"]
  }
);

// Stream validation
const validatedStream = streamDataUtils.validate(stream, {
  schema: streamDataSchema,
  onValidationError: (error) => {
    console.error("Stream data validation failed:", error);
  }
});
```

## Metadata Tracking

### Rich Metadata Collection

Comprehensive metadata tracking throughout the streaming process:

```typescript
import { MetadataStream, metadataPatterns } from "@repo/ai/server/streaming";

const metadata = new MetadataStream();

// Initialize with model information
metadata.setModelMetadata({
  name: "Claude 3.5 Sonnet",
  version: "20241022",
  provider: "anthropic",
  temperature: 0.7,
  maxTokens: 4000
});

// Track performance metrics
const stream = await streamText({
  model: createAnthropicModel("claude-3-5-sonnet-20241022"),
  messages: [{ role: "user", content: "Analyze market data" }],
  onChunk: ({ chunk, usage }) => {
    metadata.processChunk(chunk);

    if (usage) {
      metadata.updateUsage(usage.promptTokens, true);
      metadata.updateUsage(usage.completionTokens, false);
    }
  },
  onFinish: ({ usage, finishReason }) => {
    metadata.finalize(usage);

    const finalMetadata = metadata.getMetadata();
    console.log("Stream completed:", {
      duration: finalMetadata.timing.totalDuration,
      tokens: finalMetadata.usage.totalTokens,
      cost: finalMetadata.usage.estimatedCost,
      quality: finalMetadata.quality
    });
  }
});
```

### Specialized Metadata Collectors

Domain-specific metadata collection:

```typescript
// RAG-specific metadata
const ragCollector = metadataPatterns.createRAGMetadataCollector();
ragCollector.addSearchResults([
  { id: "doc1", score: 0.95, content: "Renewable energy market..." },
  { id: "doc2", score: 0.87, content: "Solar power adoption..." }
]);
ragCollector.setConfidence(0.92);

// Performance tracking
const perfCollector = metadataPatterns.createPerformanceMetadata();
perfCollector.checkpoint("data-loading");
perfCollector.recordLatency("vector-search", 150);
perfCollector.checkpoint("analysis-complete");

// Quality assessment
const qualityCollector = metadataPatterns.createQualityMetadata();
qualityCollector.addQualityCheck("factual-accuracy", true, 0.95);
qualityCollector.addQualityCheck("completeness", true, 0.88);
qualityCollector.addQualityCheck("relevance", true, 0.91);
```

### Metadata Transformers

Process and enhance metadata:

```typescript
import { createMetadataTransformer } from "@repo/ai/server/streaming";

const metadataTransformer = createMetadataTransformer(metadata);

const enhancedStream = originalStream
  .pipeThrough(metadataTransformer)
  .pipeThrough(
    new TransformStream({
      transform(chunk, controller) {
        // Add custom metadata processing
        const enrichedChunk = {
          ...chunk,
          timestamp: Date.now(),
          chunkId: generateId(),
          metadata: metadata.getMetadata()
        };

        controller.enqueue(enrichedChunk);
      }
    })
  );
```

## Stream Control

### Interruption and Resumption

Control stream execution with pause, resume, and interrupt capabilities:

```typescript
import {
  StreamInterruptionController,
  createInterruptibleStream,
  interruptionPatterns
} from "@repo/ai/server/streaming";

const controller = new StreamInterruptionController();

// Create interruptible stream
const interruptibleStream = createInterruptibleStream(
  originalStream,
  controller
);

// Set up interruption patterns
const timeoutInterruption = interruptionPatterns.createTimeoutInterruption(
  controller,
  30000
);
const tokenLimitInterruption =
  interruptionPatterns.createTokenLimitInterruption(controller, 5000);
const userInterruption =
  interruptionPatterns.createUserInterruption(controller);

// Control stream execution
controller.onInterrupt(async (reason) => {
  console.log("Stream interrupted:", reason);

  // Save current state
  controller.saveCheckpoint("interruption", {
    reason,
    timestamp: Date.now(),
    processedChunks: chunkCount
  });

  // Cleanup resources
  await cleanupResources();
});

// User-triggered controls
document.getElementById("pause-btn").onclick = () => controller.pause();
document.getElementById("resume-btn").onclick = () => controller.resume();
document.getElementById("stop-btn").onclick = () =>
  controller.interrupt("User requested stop");
```

### Resumable Streams

Manage multiple streams with resumption capabilities:

```typescript
import {
  ResumableStreamManager,
  globalStreamManager
} from "@repo/ai/server/streaming";

const streamId = "analysis-session-123";

// Register stream for management
globalStreamManager.registerStream(streamId, controller, {
  userId: "user456",
  sessionId: "session789",
  priority: "high"
});

// Save state periodically
setInterval(() => {
  globalStreamManager.saveStreamState(streamId, {
    processedTokens: currentTokenCount,
    lastChunkId: lastChunkId,
    analysisProgress: progressPercentage
  });
}, 5000);

// Handle failures with recovery
const stream = await streamText({
  model: createAnthropicModel("claude-3-5-sonnet-20241022"),
  messages: messages,
  onError: async (error) => {
    console.error("Stream failed:", error);

    // Attempt recovery
    const savedState = globalStreamManager.getStreamState(streamId);
    if (savedState) {
      console.log("Attempting recovery from checkpoint:", savedState);
      return resumeFromCheckpoint(savedState);
    }
  }
});
```

### Graceful Shutdown

Handle application shutdown gracefully:

```typescript
import { createGracefulShutdown } from "@repo/ai/server/streaming";

const activeControllers = [controller1, controller2, controller3];
const gracefulShutdown = createGracefulShutdown(activeControllers);

// Handle shutdown signals
process.on("SIGTERM", async () => {
  console.log("Received SIGTERM, shutting down gracefully...");
  await gracefulShutdown();
  process.exit(0);
});

process.on("SIGINT", async () => {
  console.log("Received SIGINT, shutting down gracefully...");
  await gracefulShutdown();
  process.exit(0);
});
```

## Backpressure Handling

### Intelligent Flow Control

Manage high-volume streams with sophisticated backpressure control:

```typescript
import {
  BackpressureController,
  createBackpressureTransform,
  backpressurePatterns
} from "@repo/ai/server/streaming";

const backpressureController = new BackpressureController({
  highWaterMark: 100, // Pause when buffer reaches 100 items
  lowWaterMark: 50, // Resume when buffer drops to 50 items
  strategy: "throttle", // "buffer", "drop", "throttle", "block"
  maxBufferSize: 1000, // Maximum buffer capacity
  throttleMs: 10 // Throttle delay
});

// Create backpressure-aware transform
const backpressureTransform = createBackpressureTransform(
  {
    highWaterMark: 200,
    strategy: "adaptive" // Automatically adjusts based on conditions
  },
  (chunk) => {
    // Optional transformation
    return {
      ...chunk,
      processed: true,
      timestamp: Date.now()
    };
  }
);

// Apply to stream
const controlledStream = originalStream.pipeThrough(backpressureTransform);
```

### Adaptive Backpressure

Dynamic backpressure adjustment based on conditions:

```typescript
const adaptiveController = backpressurePatterns.createAdaptiveBackpressure({
  highWaterMark: 100,
  strategy: "buffer"
});

// Automatically adjusts buffer size based on fill rate
setInterval(() => {
  const status = adaptiveController.controller.getStatus();
  console.log("Buffer status:", {
    size: status.bufferSize,
    fillPercentage: status.fillPercentage,
    canWrite: status.canWrite
  });
}, 1000);
```

### Prioritized Backpressure

Handle streams with different priorities:

```typescript
const prioritizedController =
  backpressurePatterns.createPrioritizedBackpressure({
    maxBufferSize: 500
  });

// High priority chunks are processed first
await prioritizedController.write({
  data: "Critical analysis result",
  priority: 10
});

await prioritizedController.write({
  data: "Background information",
  priority: 3
});

// Check queue sizes by priority
const queueSizes = prioritizedController.getQueueSizes();
console.log("Queue sizes:", queueSizes); // { 10: 1, 3: 1 }
```

### Rate-Limited Streaming

Control streaming rate with time windows:

```typescript
const rateLimitedController = backpressurePatterns.createTimeWindowBackpressure(
  1000, // 1 second window
  50 // max 50 items per second
);

const rateLimitedStream = new ReadableStream({
  async start(controller) {
    for (const item of dataItems) {
      const success = await rateLimitedController.write(item);

      if (!success) {
        // Rate limit exceeded, wait or skip
        await new Promise((resolve) => setTimeout(resolve, 100));
        continue;
      }

      const processedItem = rateLimitedController.read();
      if (processedItem) {
        controller.enqueue(processedItem);
      }
    }
  }
});
```

## Performance Optimization

### Stream Performance Analysis

Monitor and optimize stream performance:

```typescript
import { StreamPerformanceAnalyzer } from "@repo/ai/server/streaming";

const analyzer = new StreamPerformanceAnalyzer({
  trackThroughput: true,
  trackLatency: true,
  trackMemoryUsage: true,
  samplingRate: 0.1 // Sample 10% of streams
});

const stream = await streamText({
  model: createAnthropicModel("claude-3-5-sonnet-20241022"),
  messages: messages,
  onStart: () => analyzer.recordStart("text-generation"),
  onChunk: ({ chunk }) => {
    analyzer.recordChunk("text-generation", {
      size: chunk.length,
      type: "text",
      timestamp: Date.now()
    });
  },
  onFinish: () => analyzer.recordComplete("text-generation")
});

// Get performance report
const report = analyzer.getReport();
console.log("Performance metrics:", {
  averageThroughput: report.throughput.average,
  p95Latency: report.latency.p95,
  memoryPeak: report.memory.peak,
  recommendations: report.recommendations
});
```

### Memory Management

Optimize memory usage in long-running streams:

```typescript
import { StreamMemoryManager } from "@repo/ai/server/streaming";

const memoryManager = new StreamMemoryManager({
  maxMemoryUsage: "500MB",
  gcInterval: 30000, // Garbage collect every 30 seconds
  compressionEnabled: true, // Compress old chunks
  spillToDisk: {
    enabled: true,
    threshold: "100MB",
    path: "/tmp/stream-cache"
  }
});

const memoryOptimizedStream = originalStream.pipeThrough(
  memoryManager.createTransform({
    chunkRetention: 1000, // Keep last 1000 chunks in memory
    compressionRatio: 0.7 // Compress to 70% of original size
  })
);
```

### Concurrent Stream Management

Handle multiple concurrent streams efficiently:

```typescript
import { ConcurrentStreamManager } from "@repo/ai/server/streaming";

const streamManager = new ConcurrentStreamManager({
  maxConcurrentStreams: 50,
  queueSize: 200,
  priorityScheduling: true,
  resourceLimits: {
    memoryPerStream: "50MB",
    cpuPerStream: 0.1, // 10% CPU per stream
    totalMemory: "2GB"
  }
});

// Queue streams with priority
const streamPromise = streamManager.queueStream({
  priority: "high",
  userId: "user123",
  streamConfig: {
    model: createAnthropicModel("claude-3-5-sonnet-20241022"),
    messages: messages
  }
});

// Monitor resource usage
setInterval(() => {
  const stats = streamManager.getStats();
  console.log("Stream manager stats:", {
    activeStreams: stats.activeStreams,
    queuedStreams: stats.queuedStreams,
    memoryUsage: stats.memoryUsage,
    cpuUsage: stats.cpuUsage
  });
}, 5000);
```

## Advanced Integration Patterns

### Multi-Agent Streaming

Stream from multiple agents with coordination:

```typescript
import { MultiAgentStreamCoordinator } from "@repo/ai/server/streaming";

const coordinator = new MultiAgentStreamCoordinator({
  agents: ["researcher", "analyst", "writer"],
  coordinationStrategy: "pipeline", // "parallel", "pipeline", "hierarchical"
  streamMerging: "interleave"
});

const coordinatedStream = await coordinator.createCoordinatedStream({
  task: "Create comprehensive market report",
  agentConfigs: {
    researcher: {
      model: createAnthropicModel("claude-3-5-sonnet-20241022"),
      maxSteps: 5,
      tools: researchTools
    },
    analyst: {
      model: createAnthropicModel("claude-4-opus-20250514"),
      maxSteps: 8,
      tools: analysisTools
    },
    writer: {
      model: createAnthropicModel("claude-3-5-sonnet-20241022"),
      maxSteps: 6,
      tools: writingTools
    }
  }
});

// Stream includes agent identification and coordination metadata
for await (const chunk of coordinatedStream) {
  console.log(`Agent ${chunk.agentId}: ${chunk.content}`);

  if (chunk.metadata?.coordination) {
    console.log("Coordination:", chunk.metadata.coordination);
  }
}
```

### Real-time Collaboration

Multiple users collaborating on streaming content:

```typescript
import { CollaborativeStreamManager } from "@repo/ai/server/streaming";

const collaborativeManager = new CollaborativeStreamManager({
  roomId: "analysis-session-456",
  conflictResolution: "merge", // "merge", "last-writer-wins", "voting"
  permissions: {
    user1: ["read", "write", "interrupt"],
    user2: ["read", "comment"],
    user3: ["read", "write"]
  }
});

const collaborativeStream =
  await collaborativeManager.createCollaborativeStream({
    model: createAnthropicModel("claude-3-5-sonnet-20241022"),
    messages: messages,
    onUserAction: ({ userId, action, data }) => {
      console.log(`User ${userId} performed ${action}:`, data);

      // Broadcast to other users
      collaborativeManager.broadcast({
        type: "user-action",
        userId,
        action,
        data
      });
    }
  });
```

### Stream Analytics and Monitoring

Comprehensive stream monitoring:

```typescript
import { StreamAnalytics } from "@repo/ai/server/streaming";

const analytics = new StreamAnalytics({
  metricsBackend: "prometheus", // "prometheus", "datadog", "custom"
  exportInterval: 30000, // Export metrics every 30 seconds
  dashboardConfig: {
    grafanaUrl: "https://grafana.company.com",
    dashboardId: "streaming-metrics"
  }
});

// Track custom metrics
analytics.trackCustomMetric("user_satisfaction", 0.92, {
  userId: "user123",
  sessionId: "session456",
  model: "claude-3-5-sonnet-20241022"
});

// Set up alerts
analytics.addAlert({
  name: "high-error-rate",
  condition: "error_rate > 0.05",
  actions: ["email", "slack"],
  severity: "warning"
});

analytics.addAlert({
  name: "memory-usage-critical",
  condition: "memory_usage > 0.9",
  actions: ["email", "slack", "auto-scale"],
  severity: "critical"
});
```

## Best Practices

### 1. Stream Design

- Design streams to be resumable from any point
- Implement proper error boundaries and recovery
- Use appropriate backpressure strategies for your use case
- Monitor stream performance and resource usage
- Implement graceful degradation for failures

### 2. Metadata Management

- Collect comprehensive metadata from the start
- Use structured, typed metadata formats
- Implement metadata validation and sanitization
- Consider privacy implications of metadata collection
- Archive metadata for analysis and debugging

### 3. Performance Optimization

- Profile stream performance regularly
- Use appropriate buffer sizes for your workload
- Implement memory management for long-running streams
- Monitor and optimize concurrent stream limits
- Use streaming for better user experience

### 4. Error Handling

- Implement comprehensive error recovery strategies
- Use checkpointing for long-running operations
- Provide meaningful error messages to users
- Log errors with sufficient context for debugging
- Test error scenarios thoroughly

### 5. Security

- Validate all stream inputs and outputs
- Implement proper authentication and authorization
- Use secure connections for stream transport
- Monitor for anomalous streaming patterns
- Encrypt sensitive streaming data

## Troubleshooting

### Common Issues

```typescript
// Stream stalling
const stallDetector = new StreamStallDetector({
  stallThreshold: 5000, // 5 seconds
  onStall: (streamId) => {
    console.warn(`Stream ${streamId} appears to be stalled`);
    // Implement recovery logic
  }
});

// Memory leaks
const memoryLeakDetector = new MemoryLeakDetector({
  checkInterval: 60000, // Check every minute
  threshold: "100MB", // Alert if memory grows by 100MB
  onLeak: (details) => {
    console.error("Potential memory leak detected:", details);
    // Trigger garbage collection or stream restart
  }
});

// Backpressure issues
const backpressureMonitor = new BackpressureMonitor({
  alertThreshold: 0.8, // Alert when buffer is 80% full
  onAlert: (stats) => {
    console.warn("Backpressure alert:", stats);
    // Adjust flow control or add capacity
  }
});
```

Advanced streaming provides the foundation for building production-grade AI
applications that can handle high-volume, long-running streams with
sophisticated control mechanisms and comprehensive monitoring capabilities.
