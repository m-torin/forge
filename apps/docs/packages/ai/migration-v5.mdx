---
title: AI SDK v5 Migration Guide
description: Complete migration guide for upgrading to AI SDK v5 compliance
icon: "arrow-up"
---

# AI SDK v5 Migration Guide

This document explains the changes made to achieve full AI SDK v5 compliance.

## üöÄ What Changed

### 1. **Response Normalizer - REMOVED** ‚úÖ

- **Deleted**: `src/shared/utils/response-normalizer.ts`
- **Reason**: AI SDK v5 standardizes responses across all providers
- **Action**: Remove all `normalizeResponse()` calls - use raw AI SDK responses

### 2. **Custom Logging - REPLACED** ‚úÖ

- **Deleted**: `src/shared/middleware/logging.ts`
- **Added**: `src/shared/middleware/telemetry.ts`
- **Migration**: Use v5 built-in telemetry + custom middleware

**Before (v4)**:

```typescript
import { defaultLogger } from "@repo/ai/shared";
defaultLogger.logRequest(provider, operation, options);
```

**After (v5)**:

```typescript
import { telemetryMiddleware, defaultTelemetryConfig } from "@repo/ai/shared";

// Option 1: Built-in telemetry
const result = await generateText({
  model: openai("gpt-4o"),
  experimental_telemetry: defaultTelemetryConfig
  // ...
});

// Option 2: Custom middleware
const model = wrapLanguageModel({
  model: openai("gpt-4o"),
  middleware: telemetryMiddleware
});
```

### 3. **Rate Limiting - MODERNIZED** ‚úÖ

- **Deleted**: `src/shared/middleware/rate-limiting.ts`
- **Added**: `src/shared/utils/rate-limit.ts`
- **Migration**: Use v5-compatible rate limiting patterns

**Before (v4)**:

```typescript
import { defaultRateLimiter } from "@repo/ai/shared";
const { allowed } = await defaultRateLimiter.checkRateLimit(provider);
```

**After (v5)**:

```typescript
import { aiRateLimit, withRateLimit } from '@repo/ai/shared';

// Option 1: Direct usage
const { success } = await aiRateLimit.limit(req.ip ?? 'anonymous');
if (!success) return new Response('Rate limited', { status: 429 });

// Option 2: Middleware pattern
export async function POST(req: Request) {
  return withRateLimit(req, async () => {
    // Your AI logic here
    const result = await generateText({ ... });
    return result.toUIMessageStreamResponse();
  });
}
```

### 4. **Provider Configuration - MODERNIZED** ‚úÖ

- **Added**: `src/server/providers/registry.ts`
- **Migration**: Use v5 provider registry patterns

**Before (v4)**:

```typescript
import { createAnthropicModel } from "@repo/ai/server";
const model = createAnthropicModel("claude-3-sonnet");
```

**After (v5)**:

```typescript
import { registry, models } from "@repo/ai/server";

// Option 1: Registry access
const model = registry.languageModel("anthropic:sonnet");

// Option 2: Helper functions
const model = models.language.claude();

// Option 3: Direct model helper
const model = getModel("anthropic", "sonnet");
```

## üéØ New v5 Patterns

### Provider Registry

```typescript
import { registry, models } from "@repo/ai/server";

// Access any configured model
const gpt4 = registry.languageModel("openai:gpt-4o");
const claude = registry.languageModel("anthropic:sonnet");
const gemini = registry.languageModel("google:gemini-pro");

// Use helper shortcuts
const bestModel = models.language.best();
const fastModel = models.language.fast();
const reasoningModel = models.language.reasoning();
```

### Custom Model Configuration

```typescript
// Models with specific settings are pre-configured
const reasoningModel = models.language.reasoning(); // OpenAI with high reasoning effort
const creativeModel = registry.languageModel("openai:gpt-4o-creative"); // High temperature
const claudeReasoning = models.language.claudeReasoning(); // Anthropic with thinking enabled
```

### Telemetry Integration

```typescript
// Built-in telemetry (recommended)
const result = await generateText({
  model: models.language.best(),
  experimental_telemetry: { isEnabled: true },
  prompt: "Your prompt here"
});

// Access telemetry data
console.log(result.usage); // Token usage
console.log(result.response.timestamp); // Response metadata
```

### Rate Limiting in API Routes

```typescript
// Next.js API route with rate limiting
export async function POST(req: Request) {
  return withRateLimit(req, async () => {
    const result = await streamText({
      model: models.language.best(),
      messages: await req.json()
    });

    return result.toUIMessageStreamResponse();
  });
}
```

## üîß Backward Compatibility

All existing APIs continue to work, but show deprecation warnings:

```typescript
// Still works, but deprecated
import { createAnthropicModel } from "@repo/ai/server";
const model = createAnthropicModel("claude-3-sonnet");

// New recommended approach
import { models } from "@repo/ai/server";
const model = models.language.claude();
```

## üì¶ Dependencies

No new dependencies were added. The implementation uses:

- Existing `ai` package (v5 features)
- Existing `@ai-sdk/*` providers
- Mock Redis for development (no Upstash required)

## üö® Breaking Changes

### Removed Functions

- `normalizeResponse()` - Use raw AI SDK responses
- `AILogger` class - Use `experimental_telemetry` or `telemetryMiddleware`
- `AIRateLimiter` class - Use `aiRateLimit` utility

### Import Changes

```typescript
// ‚ùå These imports no longer work
import { normalizeResponse } from "@repo/ai/shared";
import { defaultLogger } from "@repo/ai/shared";
import { defaultRateLimiter } from "@repo/ai/shared";

// ‚úÖ Use these instead
import { telemetryMiddleware } from "@repo/ai/shared";
import { aiRateLimit } from "@repo/ai/shared";
import { registry, models } from "@repo/ai/server";
```

## üéØ Centralized Model Configuration

The v5 migration introduces a centralized model registry that serves as the
single source of truth for all model configurations.

### Model Registry Migration

**Before (Hardcoded Models)**:

```typescript
// ‚ùå Old approach - hardcoded model names
const model = anthropic("claude-3-5-sonnet-20241022");
const gpt = openai("gpt-4o");
const reasoning = anthropic("claude-4-opus-20250514", {
  headers: { "anthropic-beta": "interleaved-thinking-2025-05-14" }
});
```

**After (Centralized Registry)**:

```typescript
// ‚úÖ New approach - centralized configuration
import { getBestModelForTask, selectModel } from "@repo/ai/models";
import { registry, models } from "@repo/ai/server";

// Task-based selection
const chatModel = getBestModelForTask("chat");
const reasoningModel = getBestModelForTask("reasoning");

// Strategic selection with user tiers
const model = selectModel({
  strategy: "balanced",
  userTier: "pro",
  fallbackEnabled: true
});

// Use with registry (reasoning headers applied automatically)
const aiModel = registry.languageModel(`anthropic:${reasoningModel}`);
```

### Key Benefits

1. **Automatic Reasoning Configuration** - Headers and budget tokens applied
   automatically
2. **Dynamic Model Selection** - Models selected based on availability and user
   tier
3. **Single Source of Truth** - All model metadata in one location
4. **Easy Updates** - Add new models without changing application code

## ‚úÖ Benefits

1. **Full AI SDK v5 compliance** - Latest features and patterns
2. **Better performance** - Built-in optimizations
3. **Improved telemetry** - OpenTelemetry integration
4. **Standardized responses** - No custom normalization needed
5. **Modern rate limiting** - Industry-standard patterns
6. **Flexible provider configuration** - Easy to add new providers
7. **Centralized model management** - Single source of truth for all models
8. **Automatic reasoning support** - Headers and configuration applied
   automatically
9. **Dynamic model selection** - Task-based and tier-aware model selection
10. **Future-proof architecture** - Easy to add new models and providers

## üéØ Next Steps

1. Update your applications to use the new patterns
2. Remove any direct usage of deleted functions
3. Migrate from hardcoded model names to centralized selection
4. Test rate limiting with your load patterns
5. Monitor telemetry data in your observability stack
6. Use the centralized model registry for all model operations
7. Update any custom model configurations to use the registry

### Migration Checklist

- [ ] Replace hardcoded model IDs with `getBestModelForTask()`
- [ ] Update provider configurations to use `registry.languageModel()`
- [ ] Use `selectModel()` for user-tier-aware selection
- [ ] Check model availability with `isModelAvailable()`
- [ ] Remove manual reasoning header configuration
- [ ] Update imports to use `@repo/ai/models` and `@repo/ai/server`
- [ ] Test with different API key configurations

The migration maintains full backward compatibility while providing access to
all AI SDK v5 features and the new centralized model configuration system.
